{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z4udhSnyN3l1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e546e9a4-68df-4ff2-a423-d04e4d37705a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/Kaggle\"\n",
        "# /content/drive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive"
      ],
      "metadata": {
        "id": "J1X4E0v4N42v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:39.920290Z",
          "iopub.execute_input": "2023-01-18T08:52:39.921409Z",
          "iopub.status.idle": "2023-01-18T08:52:39.928501Z",
          "shell.execute_reply.started": "2023-01-18T08:52:39.921379Z",
          "shell.execute_reply": "2023-01-18T08:52:39.927486Z"
        },
        "trusted": true,
        "id": "PhVsCvJ7NMrT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG,self).__init__()\n",
        "        \n",
        "        ## using multiple layers since the early ones measure relatively lower level features like edges and later layers which measure high level features\n",
        "        ## this helps the neural network to take both lower level and higher level correlations in account when computing style\n",
        "        #self.chosen_features = ['0','5','10','19','28'] ## this is usually suggested \n",
        "        \n",
        "        self.chosen_features = ['3','8','17','26'] ## However, I got better result with these\n",
        "        \n",
        "        self.model = models.vgg19(pretrained=True).features[:29] #dropping the classification head as well as a few convolution layers\n",
        "        \n",
        "    def forward(self,x):\n",
        "        features =[]\n",
        "        \n",
        "        for layer_num, layer in enumerate(self.model):\n",
        "            x= layer(x)\n",
        "            \n",
        "            if str(layer_num) in self.chosen_features:\n",
        "                features.append(x)\n",
        "        \n",
        "        return features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:39.930016Z",
          "iopub.execute_input": "2023-01-18T08:52:39.930708Z",
          "iopub.status.idle": "2023-01-18T08:52:39.941004Z",
          "shell.execute_reply.started": "2023-01-18T08:52:39.930667Z",
          "shell.execute_reply": "2023-01-18T08:52:39.939957Z"
        },
        "trusted": true,
        "id": "p85icfzENMrT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).unsqueeze(0) ## adding additional dimenstion for the batch size\n",
        "    return image.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:39.942357Z",
          "iopub.execute_input": "2023-01-18T08:52:39.942926Z",
          "iopub.status.idle": "2023-01-18T08:52:39.951708Z",
          "shell.execute_reply.started": "2023-01-18T08:52:39.942810Z",
          "shell.execute_reply": "2023-01-18T08:52:39.950657Z"
        },
        "trusted": true,
        "id": "T-ZWPTT0NMrU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "image_size_h = 300\n",
        "image_size_w = 450"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:39.955192Z",
          "iopub.execute_input": "2023-01-18T08:52:39.955549Z",
          "iopub.status.idle": "2023-01-18T08:52:39.964480Z",
          "shell.execute_reply.started": "2023-01-18T08:52:39.955519Z",
          "shell.execute_reply": "2023-01-18T08:52:39.963505Z"
        },
        "trusted": true,
        "id": "MQh6X6nBNMrU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size_h,image_size_w)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:39.966086Z",
          "iopub.execute_input": "2023-01-18T08:52:39.966576Z",
          "iopub.status.idle": "2023-01-18T08:52:39.975523Z",
          "shell.execute_reply.started": "2023-01-18T08:52:39.966542Z",
          "shell.execute_reply": "2023-01-18T08:52:39.974202Z"
        },
        "trusted": true,
        "id": "06jLWGL9NMrU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_img = load_image(\"/content/drive/MyDrive/Kaggle/content images/best bird.jpeg\")\n",
        "style_img= load_image(\"/content/drive/MyDrive/Kaggle/cstyle images/kathhakali.jpeg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:39.978660Z",
          "iopub.execute_input": "2023-01-18T08:52:39.978955Z",
          "iopub.status.idle": "2023-01-18T08:52:40.053391Z",
          "shell.execute_reply.started": "2023-01-18T08:52:39.978930Z",
          "shell.execute_reply": "2023-01-18T08:52:40.052371Z"
        },
        "trusted": true,
        "id": "NWNAW0QpNMrU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freezing the model's parameters\n",
        "model = VGG().to(device).requires_grad_(False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:40.054948Z",
          "iopub.execute_input": "2023-01-18T08:52:40.055362Z",
          "iopub.status.idle": "2023-01-18T08:52:41.973343Z",
          "shell.execute_reply.started": "2023-01-18T08:52:40.055328Z",
          "shell.execute_reply": "2023-01-18T08:52:41.972331Z"
        },
        "trusted": true,
        "id": "0CV-RGyYNMrV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:41.974844Z",
          "iopub.execute_input": "2023-01-18T08:52:41.975571Z",
          "iopub.status.idle": "2023-01-18T08:52:41.982531Z",
          "shell.execute_reply.started": "2023-01-18T08:52:41.975529Z",
          "shell.execute_reply": "2023-01-18T08:52:41.981101Z"
        },
        "trusted": true,
        "id": "FjqBaaGmNMrV",
        "outputId": "1cc391f1-3592-44bc-b9ed-f2816062d474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## since I have frozen the model, the only thing that needs to be changed is the generated image\n",
        "\n",
        "generated = original_img.clone().requires_grad_(True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:41.986480Z",
          "iopub.execute_input": "2023-01-18T08:52:41.986900Z",
          "iopub.status.idle": "2023-01-18T08:52:41.995471Z",
          "shell.execute_reply.started": "2023-01-18T08:52:41.986865Z",
          "shell.execute_reply": "2023-01-18T08:52:41.994463Z"
        },
        "trusted": true,
        "id": "p4iaC-hYNMrW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## using the noise vector requires more computation as well as a better hardware to give a satisfiable result\n",
        "\n",
        "#generated = torch.randn(original_img.shape, device=device, requires_grad=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:41.997093Z",
          "iopub.execute_input": "2023-01-18T08:52:41.997567Z",
          "iopub.status.idle": "2023-01-18T08:52:42.006516Z",
          "shell.execute_reply.started": "2023-01-18T08:52:41.997531Z",
          "shell.execute_reply": "2023-01-18T08:52:42.005415Z"
        },
        "trusted": true,
        "id": "rI4RDtXINMrW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "total_steps = 60000\n",
        "learning_rate = 0.0001\n",
        "alpha = 1\n",
        "beta = 0.01\n",
        "optimizer = optim.Adam([generated], lr= learning_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:42.008178Z",
          "iopub.execute_input": "2023-01-18T08:52:42.008562Z",
          "iopub.status.idle": "2023-01-18T08:52:42.018535Z",
          "shell.execute_reply.started": "2023-01-18T08:52:42.008521Z",
          "shell.execute_reply": "2023-01-18T08:52:42.017534Z"
        },
        "trusted": true,
        "id": "ESM4iFzUNMrX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Style is defined as correlation between activations across channels.\n",
        "\n",
        "Correlation tells us which of the high level features tend to occur together or never occur together. <br> Here, we use the degree of correlation between channels as a measure of style. We construct a style loss which minimizes the Gram Matrix(which is a correlation matrix) of the style and the generated images so that the generated image learns a similar correlation between the activations of the style in a particular layer as the style.\n",
        "<br>\n",
        "Here, the Gram Matrix is calculating the unnormalized Cross covariance, which is used here as a proxy for correlation."
      ],
      "metadata": {
        "id": "_FLXAsQu7PaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(total_steps):\n",
        "    generated_features = model(generated)\n",
        "    original_img_features = model(original_img)\n",
        "    style_features = model(style_img)\n",
        "    # During each step, the code is passing the generated, original, and style images through the model (CNN) to generate their respective features.\n",
        "\n",
        "    style_loss = 0\n",
        "    original_loss =0\n",
        "    \n",
        "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
        "        batch_size , channel, height, width = gen_feature.shape\n",
        "        \n",
        "        original_loss += torch.mean((gen_feature-orig_feature)**2) #also works with Mean Absolute Error loss\n",
        "        \n",
        "        ## Gram Matrix for Generated image\n",
        "        \n",
        "        G= gen_feature.view(channel, height*width).mm(gen_feature.view(channel,height*width).t())\n",
        "        \n",
        "        ## Gram Matrix for Style image\n",
        "\n",
        "        S= style_feature.view(channel, height*width).mm(style_feature.view(channel,height*width).t())\n",
        "\n",
        "        style_loss +=torch.mean((G-S)**2)\n",
        "        \n",
        "    total_loss = alpha*original_loss +beta*style_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if step % 200 == 0:\n",
        "        print(total_loss)\n",
        "        save_image(generated,f\"generated{step}.jpeg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-18T08:52:42.020151Z",
          "iopub.execute_input": "2023-01-18T08:52:42.020574Z",
          "iopub.status.idle": "2023-01-18T09:59:23.640825Z",
          "shell.execute_reply.started": "2023-01-18T08:52:42.020507Z",
          "shell.execute_reply": "2023-01-18T09:59:23.639132Z"
        },
        "trusted": true,
        "id": "La9ZiiM4NMrX",
        "outputId": "8f1ef91c-36f6-43cc-d6b5-2ef3ecb7d525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3671741., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2616303.7500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1716918., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1104795.7500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(736925.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(522796.4688, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(396460.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(318998.9688, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(267941.0625, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(231110.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(202618.0312, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(179499.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(160133.2344, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(143549.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(129150.6953, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(116476.1641, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(105204.8984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(95080.1875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(85962.0703, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(77747.3281, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(70349.0703, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(63712.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(57767.5312, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(52450.6211, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(47708.0664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(43480.4180, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(39706.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(36314.2305, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(33269.5273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(30525.9004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(28031.9727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(25759.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(23694.8359, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(21818.5625, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20120.7520, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(18579.5469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(17179.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(15910.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(14753.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(13691.9180, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(12731.3779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11862.3291, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11064.3486, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(10327.2520, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9648.7725, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9029.1475, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8461.4766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(7944.5610, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(7470.1982, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(7033.3174, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6629.9346, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6264.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5925.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5609.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5316.3218, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5044.4932, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4792.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4558.9961, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4343.9512, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4142.3081, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3952.6357, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3774.1665, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3609.1516, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3456.3823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3313.0452, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3178.9663, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3053.9438, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2934.6323, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2820.5457, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2714.7505, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2615.4756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2522.8672, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2434.8694, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2351.1882, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2272.4243, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2198.4167, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2128.1316, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(2061.5669, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1999.8333, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1940.7961, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1885.3729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1833.2056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1784.2021, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1737.2590, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1693.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1652.2435, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1613.4943, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1576.5975, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1541.8020, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1509.0549, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1479.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1451.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1424.7609, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1399.6057, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1375.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1352.9462, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1331.4760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1311.3114, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1291.9331, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1273.3397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1255.4989, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1237.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1220.8726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1204.4253, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1189.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1174.5746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1160.5499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1147.0554, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1133.9949, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(1121.3528, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b1b6d5110aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgenerated_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moriginal_img_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstyle_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# During each step, the code is passing the generated, original, and style images through the model (CNN) to generate their respective features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-9ed8199e3ff8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchosen_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}